{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation and Tagging of Text\n",
    "\n",
    "In order to classify and analyze a body of text in a more granular fashion, it is necessary to consider how to break it into individual sentences and words or \"tokens\".  Broadly then there are two tasks:\n",
    "+ Sentence Tokenization\n",
    "+ Word Tokenization\n",
    "\n",
    "To go beyond counting the frequency or occurence of actual words we need to classify words in general categories that signify their part in the construct of the sentence - for instance Noun, Verb Adjective etc.  This is generally known as \n",
    "+ Part of Speech or POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenisation ##\n",
    "The default Sentence Tokenizer is the `PunktSentenceTokenizer` from the `nltk.tokenize.punkt` module.  \n",
    "\n",
    "In the example below (taken from James Joyce's Ulysses), we load the nltk library and process a block of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "ulysses = \"Mrkgnao! the cat said loudly. She blinked up out of her avid shameclosing eyes, mewing \\\n",
    "plaintively and long, showing him her milkwhite teeth. He watched the dark eyeslits narrowing \\\n",
    "with greed till her eyes were green stones. Then he went to the dresser, took the jug Hanlon's\\\n",
    "milkman had just filled for him, poured warmbubbled milk on a saucer and set it slowly on the floor.\\\n",
    "â€” Gurrhr! she cried, running to lap.\"\n",
    "\n",
    "doc = nltk.sent_tokenize(ulysses)\n",
    "for s in doc:\n",
    "    print(\">\",s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even in this very simple example, we can see that the results are not always perfect depending on the style of the text and variations from the style of the Corpus that was used to develop the tokenisation approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenisation##\n",
    "There are many methods for tokenising text into words.  The default [Penn Treebank Tokeniser](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) is the tokeniser based on the [Penn TreeBank](https://web.archive.org/web/19970614160127/http://www.cis.upenn.edu:80/~treebank/) Corpus.  A few examples of different tokenisers giving different results are listed below:\n",
    "\n",
    "+ TreebankWordTokenizer\n",
    "+ WordPunctTokenizer\n",
    "+ WhitespaceTokenize\n",
    "\n",
    "We can see a simple illustration of the impact of chosing a different tokenisation method by looking at the different results we get for a simple sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"Mary had a little lamb it's fleece was white as snow.\"\n",
    "# Default Tokenisation\n",
    "tree_tokens = word_tokenize(sentence)   # nltk.download('punkt') for this\n",
    "\n",
    "# Other Tokenisers\n",
    "punct_tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "punct_tokens = punct_tokenizer.tokenize(sentence)\n",
    "\n",
    "space_tokenizer = nltk.tokenize.SpaceTokenizer()\n",
    "space_tokens = space_tokenizer.tokenize(sentence)\n",
    "\n",
    "print(\"DEFAULT: \", tree_tokens)\n",
    "print(\"PUNCT  : \", punct_tokens)\n",
    "print(\"SPACE  : \", space_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part of Speech Tagging ##\n",
    "\n",
    "For each word-token the nltk pos_tag method can be used to classify its Part of Speech (POS), automating the classification of words into their parts of speech and labeling them accordingly.\n",
    "\n",
    "The outcome depends on how the sentence has been split up into individual tokens and which Tokensizer and Corpus the POS-tagger has been trained against:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = nltk.pos_tag(tree_tokens)\n",
    "print(pos)\n",
    "pos_space = nltk.pos_tag(space_tokens)\n",
    "print(pos_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PoS Tag Descriptions ###\n",
    "CC | Coordinating conjunction  \n",
    "CD | Cardinal number  \n",
    "DT | Determiner  \n",
    "EX | Existential there  \n",
    "FW | Foreign word  \n",
    "IN | Preposition or subordinating conjunction  \n",
    "JJ | Adjective  \n",
    "JJR | Adjective, comparative  \n",
    "JJS | Adjective, superlative  \n",
    "LS | List item marker  \n",
    "MD | Modal  \n",
    "NN | Noun, singular or mass  \n",
    "NNS | Noun, plural  \n",
    "NNP | Proper noun, singular  \n",
    "NNPS | Proper noun, plural  \n",
    "PDT | Predeterminer  \n",
    "POS | Possessive ending  \n",
    "PRP | Personal pronoun  \n",
    "PRP\\$ | Possessive pronoun  \n",
    "RB | Adverb  \n",
    "RBR | Adverb, comparative  \n",
    "RBS | Adverb, superlative  \n",
    "RP | Particle  \n",
    "SYM | Symbol  \n",
    "TO | to  \n",
    "UH | Interjection  \n",
    "VB | Verb, base form  \n",
    "VBD | Verb, past tense  \n",
    "VBG | Verb, gerund or present participle  \n",
    "VBN | Verb, past participle  \n",
    "VBP | Verb, non-3rd person singular present  \n",
    "VBZ | Verb, 3rd person singular present  \n",
    "WDT | Wh-determiner  \n",
    "WP | Wh-pronoun  \n",
    "WP$ | Possessive wh-pronoun  \n",
    "WRB | Wh-adverb   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The naming convention of the PoS tags makes it easy to use regular expressions to extract classes of word-type (i.e. all the Nouns or Verbs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile(\"^N.*\")\n",
    "nouns = []\n",
    "for l in pos:\n",
    "    if regex.match(l[1]):\n",
    "        nouns.append(l[0])\n",
    "print(\"Nouns:\", nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatizing ##\n",
    "Striping off the suffixes from words is known as *stemming*.  \n",
    "Mapping a word to a known dictionary word is know as *lemmatization*\n",
    "\n",
    "There are multiple Stemming methods available and the the NLTK book references a few methods in particular:  \n",
    "+ The Porter Stemmer - see https://tartarus.org/martin/PorterStemmer/   \n",
    "+ Lancaster Stemmer - (Chris Paice, University of Lancaster)\n",
    "additionally the \n",
    "+ Snowball Stemmer - \"Porter 2\" developed by Martin Porter\n",
    "is generally considered the de-facto optimal Stemmer\n",
    "\n",
    "A list of other stemming methods can be found here: http://www.nltk.org/api/nltk.stem.html.  Current Stemming and \"Lemming\" techniques are an inexact process as things currently stand.\n",
    "\n",
    "#### Stemming Example ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "snowball = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "\n",
    "print([porter.stem(t) for t in tree_tokens])\n",
    "print([lancaster.stem(t) for t in tree_tokens])\n",
    "print([snowball.stem(t) for t in tree_tokens])\n",
    "\n",
    "sentence2 = \"When I was going into the woods I saw a bear lying asleep on the forest floor\"\n",
    "tokens2 = word_tokenize(sentence2)\n",
    "\n",
    "print(\"\\n\",sentence2)\n",
    "for stemmer in [porter, lancaster, snowball]:\n",
    "    print([stemmer.stem(t) for t in tokens2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing Example ####\n",
    "Lemmatization aims to achieve a similar base \"stem\" for a word, but aims to derive the  genuine dictionary root word, not just a trunctated version of the word.  \n",
    "\n",
    "The default lemmatization method with the Python NLTK is the WordNet lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "tokens2_pos = nltk.pos_tag(tokens2)  #nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "print([wnl.lemmatize(t) for t in tree_tokens])\n",
    "\n",
    "print([wnl.lemmatize(t) for t in tokens2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary ###\n",
    "By Tokenising text into sentences and words we can go beyond counting the frequency or occurence of actual words and instead classify words by a classification type (i.e. we can identify common features in the text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Investigation ##\n",
    "Optional further work and experimentation:\n",
    "    \n",
    "#### 1. Regular Expressions and POS patterns ####\n",
    "Consider how to extract \"phrase-chunks\" based on regular expressions.  \n",
    "\n",
    "See this Stack Overflow thread for one idea:\n",
    "http://stackoverflow.com/questions/34090734/how-to-use-nltk-regex-pattern-to-extract-a-specific-phrase-chunk\n",
    "    \n",
    "Consider the pitfalls and complexities for different sentence constructs with essentially the same meaning.\n",
    "\n",
    "#### 2. Experiment with the POS Tags ####\n",
    "Try tokenising the sentance:  \n",
    "*\"When I was going into the woods I saw a bear lying asleep on the forest floor\"*  \n",
    "and note any inaccuriacies in the PoS classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
